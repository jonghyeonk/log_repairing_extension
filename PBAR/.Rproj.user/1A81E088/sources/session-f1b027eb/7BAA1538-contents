#For PM4PY

import os
import tensorflow as tf
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
if tf.test.gpu_device_name():
    print('GPU found')
else:
    print("No GPU found")
import pm4py
from pm4py.objects.conversion.log import converter as log_converter
from pm4py.algo.discovery.inductive import algorithm as inductive_miner
from pm4py.algo.conformance.alignments.petri_net import algorithm
import pandas as pd
import numpy
import datetime
from pm4py.util import exec_utils
import pkgutil

rate = ['0.10', '0.25', '0.50', '1.00']

##
before = pd.read_csv("~\\OneDrive\\바탕 화면\\볼차노\\PBAR_extension\\alignment\\normaldata\\Small.csv")
after = pd.read_csv("~\\OneDrive\\바탕 화면\\볼차노\\PBAR_extension\\alignment\\anomaly_v2\\Small_"+ rate[i] + ".csv")

##
before = before[['Case ID', 'Activity', "Complete Timestamp", "Variant"]]
before = before.rename(columns={"Case ID": "case:concept:name", "Activity": "concept:name",
                       "Complete Timestamp": "time:timestamp", "Variant": "Resource"})
after = after[['Case', 'Activity', "Timestamp", "resource_anomaly_type"]]
after = after.rename(columns={"Case": "case:concept:name", "Activity": "concept:name",
                       "Timestamp": "time:timestamp", "resource_anomaly_type": "label"})

clean = after[after.label.isin(['normal']) ]
anomaly = after[~after.label.isin(['normal']) ]
clean = clean.reset_index(drop=True)
anomaly = anomaly.reset_index(drop=True)

clean = clean[["case:concept:name", "concept:name","time:timestamp" ]]
anomaly = anomaly[["case:concept:name", "concept:name","time:timestamp" ]]

correct_align = before[before["case:concept:name"].isin( anomaly["case:concept:name"].unique()) ]
correct_align = correct_align[["case:concept:name", "concept:name","time:timestamp" ]]

anomalyid = anomaly["case:concept:name"].unique()
##


log = log_converter.apply(clean)
log2 = log_converter.apply(anomaly)

#print('start_processdiscovery')
start= datetime.datetime.now()

net, initial_marking, final_marking = inductive_miner.apply(log)

#net, initial_marking, final_marking = inductive_miner.apply(log,parameters=
#                                                            {inductive_miner.Variants.IMf.value.Parameters.NOISE_THRESHOLD: 0.2})
#print('start_alignment')
alignments = algorithm.apply_log(log2, net, initial_marking, final_marking)


# alignments = algorithm.apply_log(log2, net, initial_marking, final_marking, parameters=  {algorithm.Parameters.PARAM_MAX_ALIGN_TIME_TRACE: 10} )

#
end= datetime.datetime.now()
#print('finished')
cost = list()
cost.append(int((end-start).total_seconds())/60)

cases = anomaly['case:concept:name'].unique()
length = len(cases)
w=0
case_l = list()
case2_l = list()
act3_l = list()

score=0
for i in alignments:
    if i is None:
        print("None!!")
        #print(w)


    else:
        act_l = list()
        act2_l = list()
        org = correct_align[correct_align["case:concept:name"].isin([anomalyid[w]])]
        act_org = org["concept:name"].values.tolist()
        caseid = anomalyid[w]
        for k in i['alignment']:
            if (k[1] != None) and (k[1] != '>>'):
                act2_l.append(k[1])
                case2_l.append(caseid)
                act3_l.append(k[1])
            if k[0] != '>>':
                act_l.append(k[0])
                case_l.append(caseid)
        w = w + 1
        if act_org != act2_l:
            score = score + 1




d = {'Case':case2_l,'Activity':act3_l}
df = pd.DataFrame(d)



print(1-score/length)

df.to_csv("Wide1_align.csv")





#########################################################
#########################################################


##
before = pd.read_csv("~\\PycharmProjects\\alignment\\normaldata\\Wide.csv")
after = pd.read_csv("~\\PycharmProjects\\alignment\\preprocessed\\recon_Wide2.csv")

##
before = before[['Case ID', 'Activity', "Complete Timestamp", "Variant"]]
before = before.rename(columns={"Case ID": "case:concept:name", "Activity": "concept:name",
                       "Complete Timestamp": "time:timestamp", "Variant": "Resource"})
after = after[['Case', 'Activity', "Timestamp", "resource_anomaly_type"]]
after = after.rename(columns={"Case": "case:concept:name", "Activity": "concept:name",
                       "Timestamp": "time:timestamp", "resource_anomaly_type": "label"})

clean = after[after.label.isin(['normal']) ]
anomaly = after[~after.label.isin(['normal']) ]
clean = clean.reset_index(drop=True)
anomaly = anomaly.reset_index(drop=True)

clean = clean[["case:concept:name", "concept:name","time:timestamp" ]]
anomaly = anomaly[["case:concept:name", "concept:name","time:timestamp" ]]

correct_align = before[before["case:concept:name"].isin( anomaly["case:concept:name"].unique()) ]
correct_align = correct_align[["case:concept:name", "concept:name","time:timestamp" ]]

anomalyid = anomaly["case:concept:name"].unique()
##


log = log_converter.apply(clean)
log2 = log_converter.apply(anomaly)

#print('start_processdiscovery')
start= datetime.datetime.now()

net, initial_marking, final_marking = inductive_miner.apply(log)

#net, initial_marking, final_marking = inductive_miner.apply(log,parameters=
#                                                            {inductive_miner.Variants.IMf.value.Parameters.NOISE_THRESHOLD: 0.2})
#print('start_alignment')
alignments = algorithm.apply_log(log2, net, initial_marking, final_marking)


# alignments = algorithm.apply_log(log2, net, initial_marking, final_marking, parameters=  {algorithm.Parameters.PARAM_MAX_ALIGN_TIME_TRACE: 10} )

#
end= datetime.datetime.now()
#print('finished')
cost.append(int((end-start).total_seconds())/60)

cases = anomaly['case:concept:name'].unique()
length = len(cases)
w=0
case_l = list()
case2_l = list()
act3_l = list()
score=0
for i in alignments:
    if i is None:
        print("None!!")
        #print(w)


    else:
        act_l = list()
        act2_l = list()
        org = correct_align[correct_align["case:concept:name"].isin([anomalyid[w]])]
        act_org = org["concept:name"].values.tolist()
        caseid = anomalyid[w]
        for k in i['alignment']:
            if (k[1] != None) and (k[1] != '>>'):
                act2_l.append(k[1])
                case2_l.append(caseid)
                act3_l.append(k[1])
            if k[0] != '>>':
                act_l.append(k[0])
                case_l.append(caseid)
        w = w + 1
        if act_org != act2_l:
            score = score + 1




d = {'Case':case2_l,'Activity':act3_l}
df = pd.DataFrame(d)


print(1-score/length)

df.to_csv("Wide2_align.csv")





#########################################################
#########################################################


##
before = pd.read_csv("~\\PycharmProjects\\alignment\\normaldata\\Wide.csv")
after = pd.read_csv("~\\PycharmProjects\\alignment\\preprocessed\\recon_Wide3.csv")

##
before = before[['Case ID', 'Activity', "Complete Timestamp", "Variant"]]
before = before.rename(columns={"Case ID": "case:concept:name", "Activity": "concept:name",
                       "Complete Timestamp": "time:timestamp", "Variant": "Resource"})
after = after[['Case', 'Activity', "Timestamp", "resource_anomaly_type"]]
after = after.rename(columns={"Case": "case:concept:name", "Activity": "concept:name",
                       "Timestamp": "time:timestamp", "resource_anomaly_type": "label"})

clean = after[after.label.isin(['normal']) ]
anomaly = after[~after.label.isin(['normal']) ]
clean = clean.reset_index(drop=True)
anomaly = anomaly.reset_index(drop=True)

clean = clean[["case:concept:name", "concept:name","time:timestamp" ]]
anomaly = anomaly[["case:concept:name", "concept:name","time:timestamp" ]]

correct_align = before[before["case:concept:name"].isin( anomaly["case:concept:name"].unique()) ]
correct_align = correct_align[["case:concept:name", "concept:name","time:timestamp" ]]

anomalyid = anomaly["case:concept:name"].unique()
##


log = log_converter.apply(clean)
log2 = log_converter.apply(anomaly)

#print('start_processdiscovery')
start= datetime.datetime.now()

net, initial_marking, final_marking = inductive_miner.apply(log)

#net, initial_marking, final_marking = inductive_miner.apply(log,parameters=
#                                                            {inductive_miner.Variants.IMf.value.Parameters.NOISE_THRESHOLD: 0.2})
#print('start_alignment')
alignments = algorithm.apply_log(log2, net, initial_marking, final_marking)


# alignments = algorithm.apply_log(log2, net, initial_marking, final_marking, parameters=  {algorithm.Parameters.PARAM_MAX_ALIGN_TIME_TRACE: 10} )

#
end= datetime.datetime.now()
#print('finished')
cost.append(int((end-start).total_seconds())/60)

cases = anomaly['case:concept:name'].unique()
length = len(cases)
w=0
case_l = list()
case2_l = list()
act3_l = list()
score=0
for i in alignments:
    if i is None:
        print("None!!")
        #print(w)


    else:
        act_l = list()
        act2_l = list()
        org = correct_align[correct_align["case:concept:name"].isin([anomalyid[w]])]
        act_org = org["concept:name"].values.tolist()
        caseid = anomalyid[w]
        for k in i['alignment']:
            if (k[1] != None) and (k[1] != '>>'):
                act2_l.append(k[1])
                case2_l.append(caseid)
                act3_l.append(k[1])
            if k[0] != '>>':
                act_l.append(k[0])
                case_l.append(caseid)
        w = w + 1
        if act_org != act2_l:
            score = score + 1




d = {'Case':case2_l,'Activity':act3_l}
df = pd.DataFrame(d)



print(1-score/length)

df.to_csv("Wide3_align.csv")





#########################################################
#########################################################


##
before = pd.read_csv("~\\PycharmProjects\\alignment\\normaldata\\Wide.csv")
after = pd.read_csv("~\\PycharmProjects\\alignment\\preprocessed\\recon_Wide4.csv")

##
before = before[['Case ID', 'Activity', "Complete Timestamp", "Variant"]]
before = before.rename(columns={"Case ID": "case:concept:name", "Activity": "concept:name",
                       "Complete Timestamp": "time:timestamp", "Variant": "Resource"})
after = after[['Case', 'Activity', "Timestamp", "resource_anomaly_type"]]
after = after.rename(columns={"Case": "case:concept:name", "Activity": "concept:name",
                       "Timestamp": "time:timestamp", "resource_anomaly_type": "label"})

clean = after[after.label.isin(['normal']) ]
anomaly = after[~after.label.isin(['normal']) ]
clean = clean.reset_index(drop=True)
anomaly = anomaly.reset_index(drop=True)

clean = clean[["case:concept:name", "concept:name","time:timestamp" ]]
anomaly = anomaly[["case:concept:name", "concept:name","time:timestamp" ]]

correct_align = before[before["case:concept:name"].isin( anomaly["case:concept:name"].unique()) ]
correct_align = correct_align[["case:concept:name", "concept:name","time:timestamp" ]]

anomalyid = anomaly["case:concept:name"].unique()
##


log = log_converter.apply(clean)
log2 = log_converter.apply(anomaly)

#print('start_processdiscovery')
start= datetime.datetime.now()

net, initial_marking, final_marking = inductive_miner.apply(log)

#net, initial_marking, final_marking = inductive_miner.apply(log,parameters=
#                                                            {inductive_miner.Variants.IMf.value.Parameters.NOISE_THRESHOLD: 0.2})
#print('start_alignment')
alignments = algorithm.apply_log(log2, net, initial_marking, final_marking)


# alignments = algorithm.apply_log(log2, net, initial_marking, final_marking, parameters=  {algorithm.Parameters.PARAM_MAX_ALIGN_TIME_TRACE: 10} )

#
end= datetime.datetime.now()
#print('finished')
cost.append(int((end-start).total_seconds())/60)

cases = anomaly['case:concept:name'].unique()
length = len(cases)
w=0
case_l = list()
case2_l = list()
act3_l = list()
score=0
for i in alignments:
    if i is None:
        print("None!!")
        #print(w)


    else:
        act_l = list()
        act2_l = list()
        org = correct_align[correct_align["case:concept:name"].isin([anomalyid[w]])]
        act_org = org["concept:name"].values.tolist()
        caseid = anomalyid[w]
        for k in i['alignment']:
            if (k[1] != None) and (k[1] != '>>'):
                act2_l.append(k[1])
                case2_l.append(caseid)
                act3_l.append(k[1])
            if k[0] != '>>':
                act_l.append(k[0])
                case_l.append(caseid)
        w = w + 1
        if act_org != act2_l:
            score = score + 1




d = {'Case':case2_l,'Activity':act3_l}
df = pd.DataFrame(d)


print(1-score/length)

df.to_csv("Wide4_align.csv")






#########################################################
#########################################################


##
before = pd.read_csv("~\\PycharmProjects\\alignment\\normaldata\\Wide.csv")
after = pd.read_csv("~\\PycharmProjects\\alignment\\preprocessed\\recon_Wide5.csv")

##
before = before[['Case ID', 'Activity', "Complete Timestamp", "Variant"]]
before = before.rename(columns={"Case ID": "case:concept:name", "Activity": "concept:name",
                       "Complete Timestamp": "time:timestamp", "Variant": "Resource"})
after = after[['Case', 'Activity', "Timestamp", "resource_anomaly_type"]]
after = after.rename(columns={"Case": "case:concept:name", "Activity": "concept:name",
                       "Timestamp": "time:timestamp", "resource_anomaly_type": "label"})

clean = after[after.label.isin(['normal']) ]
anomaly = after[~after.label.isin(['normal']) ]
clean = clean.reset_index(drop=True)
anomaly = anomaly.reset_index(drop=True)

clean = clean[["case:concept:name", "concept:name","time:timestamp" ]]
anomaly = anomaly[["case:concept:name", "concept:name","time:timestamp" ]]

correct_align = before[before["case:concept:name"].isin( anomaly["case:concept:name"].unique()) ]
correct_align = correct_align[["case:concept:name", "concept:name","time:timestamp" ]]

anomalyid = anomaly["case:concept:name"].unique()
##


log = log_converter.apply(clean)
log2 = log_converter.apply(anomaly)

#print('start_processdiscovery')
start= datetime.datetime.now()

net, initial_marking, final_marking = inductive_miner.apply(log)

#net, initial_marking, final_marking = inductive_miner.apply(log,parameters=
#                                                            {inductive_miner.Variants.IMf.value.Parameters.NOISE_THRESHOLD: 0.2})
#print('start_alignment')
alignments = algorithm.apply_log(log2, net, initial_marking, final_marking)


# alignments = algorithm.apply_log(log2, net, initial_marking, final_marking, parameters=  {algorithm.Parameters.PARAM_MAX_ALIGN_TIME_TRACE: 10} )

#
end= datetime.datetime.now()
#print('finished')
cost.append(int((end-start).total_seconds())/60)

cases = anomaly['case:concept:name'].unique()
length = len(cases)
w=0
case_l = list()
case2_l = list()
act3_l = list()
score=0
for i in alignments:
    if i is None:
        print("None!!")
        #print(w)


    else:
        act_l = list()
        act2_l = list()
        org = correct_align[correct_align["case:concept:name"].isin([anomalyid[w]])]
        act_org = org["concept:name"].values.tolist()
        caseid = anomalyid[w]
        for k in i['alignment']:
            if (k[1] != None) and (k[1] != '>>'):
                act2_l.append(k[1])
                case2_l.append(caseid)
                act3_l.append(k[1])
            if k[0] != '>>':
                act_l.append(k[0])
                case_l.append(caseid)
        w = w + 1
        if act_org != act2_l:
            score = score + 1




d = {'Case':case2_l,'Activity':act3_l}
df = pd.DataFrame(d)

print(sum(cost)/len(cost))

df.to_csv("Wide5_align.csv")


